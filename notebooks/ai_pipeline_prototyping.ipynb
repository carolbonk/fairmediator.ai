{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairMediator AI Pipeline - Prototyping Notebook\n",
    "\n",
    "This notebook provides a comprehensive development environment for the FairMediator AI pipeline.\n",
    "\n",
    "## Contents\n",
    "1. **Environment Setup** - Install dependencies, load models\n",
    "2. **HuggingFace Models** - Text classification, NER, sentiment analysis\n",
    "3. **Data Collection** - Web scraping with Crawl4AI/BeautifulSoup\n",
    "4. **Affiliation Detection** - Train and evaluate classification models\n",
    "5. **Ideology Classification** - Political leaning detection\n",
    "6. **Interactive Testing** - Widgets for real-time testing\n",
    "7. **Integration Guide** - Connect to backend/frontend\n",
    "\n",
    "**Note**: All services use FREE tier APIs (HuggingFace, Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q transformers datasets torch scikit-learn pandas numpy\n",
    "!pip install -q beautifulsoup4 aiohttp requests\n",
    "!pip install -q ipywidgets matplotlib seaborn\n",
    "!pip install -q python-dotenv pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML/NLP\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Web scraping\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../backend/.env')\n",
    "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY', '')\n",
    "\n",
    "print(\"âœ… Environment loaded successfully!\")\n",
    "print(f\"ğŸ“¦ HuggingFace API Key: {'Configured' if HUGGINGFACE_API_KEY else 'Not set (will use public models)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load HuggingFace Models\n",
    "\n",
    "We'll load three key models:\n",
    "1. **Text Classification** - Sentiment/affiliation detection\n",
    "2. **Named Entity Recognition (NER)** - Extract organizations, people\n",
    "3. **Zero-Shot Classification** - Flexible categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Text Classification Model (Sentiment)\n",
    "print(\"Loading text classification model...\")\n",
    "sentiment_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=sentiment_model_name,\n",
    "    tokenizer=sentiment_model_name\n",
    ")\n",
    "print(f\"âœ… Loaded: {sentiment_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Named Entity Recognition (NER) Model\n",
    "print(\"Loading NER model...\")\n",
    "ner_model_name = \"dslim/bert-base-NER\"\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=ner_model_name,\n",
    "    tokenizer=ner_model_name,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "print(f\"âœ… Loaded: {ner_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Zero-Shot Classification Model (for flexible categorization)\n",
    "print(\"Loading zero-shot classification model...\")\n",
    "zero_shot_model = \"facebook/bart-large-mnli\"\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=zero_shot_model\n",
    ")\n",
    "print(f\"âœ… Loaded: {zero_shot_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentiment analysis\n",
    "test_texts = [\n",
    "    \"The mediator handled our case professionally and fairly.\",\n",
    "    \"Terrible experience, completely biased towards the corporation.\",\n",
    "    \"Average service, nothing special but got the job done.\"\n",
    "]\n",
    "\n",
    "print(\"=== Sentiment Analysis Results ===\")\n",
    "for text in test_texts:\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    print(f\"\\nText: {text[:50]}...\")\n",
    "    print(f\"  Sentiment: {result['label']} (confidence: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NER for extracting organizations and people\n",
    "test_bio = \"\"\"\n",
    "John Smith is a senior partner at Morrison & Foerster LLP in San Francisco. \n",
    "He previously worked at the ACLU and served on the board of the Heritage Foundation.\n",
    "He graduated from Harvard Law School and is admitted to the California State Bar.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Named Entity Recognition Results ===\")\n",
    "entities = ner_pipeline(test_bio)\n",
    "\n",
    "# Group by entity type\n",
    "entity_groups = {}\n",
    "for ent in entities:\n",
    "    ent_type = ent['entity_group']\n",
    "    if ent_type not in entity_groups:\n",
    "        entity_groups[ent_type] = []\n",
    "    entity_groups[ent_type].append(ent['word'])\n",
    "\n",
    "for ent_type, words in entity_groups.items():\n",
    "    print(f\"\\n{ent_type}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test zero-shot classification for ideology detection\n",
    "ideology_text = \"\"\"\n",
    "The mediator has been involved in progressive causes, supporting worker rights \n",
    "and environmental regulations. They've donated to Democratic candidates.\n",
    "\"\"\"\n",
    "\n",
    "candidate_labels = [\"liberal/progressive\", \"conservative/traditional\", \"neutral/centrist\"]\n",
    "\n",
    "print(\"=== Zero-Shot Ideology Classification ===\")\n",
    "result = zero_shot_classifier(ideology_text, candidate_labels)\n",
    "\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Data Collection with Web Scraping\n",
    "\n",
    "Using BeautifulSoup + aiohttp (free, Python 3.9 compatible alternative to Crawl4AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediatorScraper:\n",
    "    \"\"\"\n",
    "    Web scraper for mediator profiles from legal databases.\n",
    "    Uses BeautifulSoup + aiohttp for async scraping.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "        }\n",
    "        self.sources = {\n",
    "            'martindale': 'https://www.martindale.com/search/attorneys',\n",
    "            'avvo': 'https://www.avvo.com/find-a-lawyer',\n",
    "            'justia': 'https://www.justia.com/lawyers'\n",
    "        }\n",
    "    \n",
    "    async def fetch_page(self, url: str) -> str:\n",
    "        \"\"\"Fetch a web page asynchronously.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url, headers=self.headers) as response:\n",
    "                return await response.text()\n",
    "    \n",
    "    def extract_text(self, html: str) -> str:\n",
    "        \"\"\"Extract clean text from HTML.\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for element in soup(['script', 'style', 'nav', 'footer']):\n",
    "            element.decompose()\n",
    "        \n",
    "        return soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    def extract_entities_from_text(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract organizations, people, and locations using NER.\"\"\"\n",
    "        entities = ner_pipeline(text[:5000])  # Limit for performance\n",
    "        \n",
    "        result = {\n",
    "            'organizations': [],\n",
    "            'people': [],\n",
    "            'locations': []\n",
    "        }\n",
    "        \n",
    "        for ent in entities:\n",
    "            if ent['entity_group'] == 'ORG':\n",
    "                result['organizations'].append(ent['word'])\n",
    "            elif ent['entity_group'] == 'PER':\n",
    "                result['people'].append(ent['word'])\n",
    "            elif ent['entity_group'] == 'LOC':\n",
    "                result['locations'].append(ent['word'])\n",
    "        \n",
    "        # Deduplicate\n",
    "        for key in result:\n",
    "            result[key] = list(set(result[key]))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def extract_contact_info(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract email, phone, and address from text.\"\"\"\n",
    "        patterns = {\n",
    "            'email': r'[\\w.-]+@[\\w.-]+\\.\\w+',\n",
    "            'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "            'hourly_rate': r'\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?(?:/hr|/hour|per hour)?'\n",
    "        }\n",
    "        \n",
    "        result = {}\n",
    "        for key, pattern in patterns.items():\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            result[key] = match.group() if match else None\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def scrape_mediator_profile(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Scrape and analyze a mediator profile.\"\"\"\n",
    "        try:\n",
    "            html = await self.fetch_page(url)\n",
    "            text = self.extract_text(html)\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'text_length': len(text),\n",
    "                'entities': self.extract_entities_from_text(text),\n",
    "                'contact': self.extract_contact_info(text),\n",
    "                'raw_text': text[:1000]  # First 1000 chars for preview\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'url': url, 'error': str(e)}\n",
    "\n",
    "# Initialize scraper\n",
    "scraper = MediatorScraper()\n",
    "print(\"âœ… MediatorScraper initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scrape a public legal profile page\n",
    "# Note: Replace with actual mediator profile URLs\n",
    "\n",
    "async def demo_scrape():\n",
    "    # Using a public Wikipedia page as demo\n",
    "    demo_url = \"https://en.wikipedia.org/wiki/Mediation\"\n",
    "    \n",
    "    print(f\"Scraping: {demo_url}\")\n",
    "    result = await scraper.scrape_mediator_profile(demo_url)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"\\nâœ… Successfully scraped {result['text_length']} characters\")\n",
    "        print(f\"\\nğŸ“ Entities found:\")\n",
    "        for ent_type, entities in result['entities'].items():\n",
    "            if entities:\n",
    "                print(f\"  {ent_type}: {', '.join(entities[:5])}{'...' if len(entities) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"âŒ Error: {result['error']}\")\n",
    "\n",
    "# Run the demo\n",
    "await demo_scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Affiliation Detection Pipeline\n",
    "\n",
    "Train a classifier to detect potential affiliations/conflicts of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data for affiliation detection\n",
    "# In production, this would come from your MongoDB database\n",
    "\n",
    "affiliation_data = [\n",
    "    # Positive examples (affiliated/conflict)\n",
    "    {\"text\": \"Currently a partner at Goldman Sachs legal division\", \"label\": 1},\n",
    "    {\"text\": \"Served on the board of directors for Acme Corporation\", \"label\": 1},\n",
    "    {\"text\": \"Former general counsel at the defendant's parent company\", \"label\": 1},\n",
    "    {\"text\": \"Married to the CEO of the opposing party's firm\", \"label\": 1},\n",
    "    {\"text\": \"Previously represented the defendant in similar matters\", \"label\": 1},\n",
    "    {\"text\": \"Owns stock in companies related to the dispute\", \"label\": 1},\n",
    "    {\"text\": \"Close personal friend of the plaintiff's attorney\", \"label\": 1},\n",
    "    {\"text\": \"Received consulting fees from related industry groups\", \"label\": 1},\n",
    "    \n",
    "    # Negative examples (no affiliation)\n",
    "    {\"text\": \"Independent mediator with 20 years of experience\", \"label\": 0},\n",
    "    {\"text\": \"No prior relationship with either party\", \"label\": 0},\n",
    "    {\"text\": \"Certified by the American Arbitration Association\", \"label\": 0},\n",
    "    {\"text\": \"Practices exclusively in family law mediation\", \"label\": 0},\n",
    "    {\"text\": \"Retired judge with impeccable neutrality record\", \"label\": 0},\n",
    "    {\"text\": \"Has never worked in the financial services industry\", \"label\": 0},\n",
    "    {\"text\": \"No business connections to either party or their counsel\", \"label\": 0},\n",
    "    {\"text\": \"Based in a different state from both parties\", \"label\": 0},\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(affiliation_data)\n",
    "print(f\"Training data: {len(df)} samples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for HuggingFace Trainer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Create HuggingFace dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(f\"âœ… Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model for fine-tuning\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./affiliation_model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured\")\n",
    "print(\"\\nâš ï¸ Note: Training with small dataset for demo. Use larger dataset in production.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (uncomment to run)\n",
    "# trainer.train()\n",
    "\n",
    "# For demo, we'll use the pre-trained zero-shot classifier instead\n",
    "print(\"Using zero-shot classification for affiliation detection (no training required)\")\n",
    "\n",
    "def detect_affiliation(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect potential affiliation/conflict of interest using zero-shot classification.\n",
    "    \"\"\"\n",
    "    labels = [\"potential conflict of interest\", \"no conflict of interest\"]\n",
    "    result = zero_shot_classifier(text, labels)\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'prediction': result['labels'][0],\n",
    "        'confidence': result['scores'][0],\n",
    "        'is_affiliated': result['labels'][0] == \"potential conflict of interest\"\n",
    "    }\n",
    "\n",
    "# Test the detector\n",
    "test_cases = [\n",
    "    \"Former partner at the defendant's law firm\",\n",
    "    \"Independent arbitrator with AAA certification\",\n",
    "    \"Owns 5% equity stake in plaintiff's company\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Affiliation Detection Results ===\")\n",
    "for text in test_cases:\n",
    "    result = detect_affiliation(text)\n",
    "    status = \"âš ï¸ CONFLICT\" if result['is_affiliated'] else \"âœ… CLEAR\"\n",
    "    print(f\"\\n{status}\")\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Ideology Classification\n",
    "\n",
    "Detect political leaning based on donations, affiliations, and public statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdeologyClassifier:\n",
    "    \"\"\"\n",
    "    Classify mediator ideology based on various signals.\n",
    "    Uses both keyword matching and ML-based classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Keyword indicators\n",
    "        self.liberal_keywords = [\n",
    "            'progressive', 'equality', 'social justice', 'environmental',\n",
    "            'civil rights', 'labor rights', 'ACLU', 'diversity', 'inclusion',\n",
    "            'worker protection', 'regulation', 'climate', 'equity'\n",
    "        ]\n",
    "        \n",
    "        self.conservative_keywords = [\n",
    "            'traditional', 'liberty', 'free market', 'constitutional',\n",
    "            'heritage foundation', 'family values', 'federalist',\n",
    "            'limited government', 'deregulation', 'individual responsibility'\n",
    "        ]\n",
    "        \n",
    "        # Organization affiliations\n",
    "        self.liberal_orgs = [\n",
    "            'ACLU', 'Sierra Club', 'Planned Parenthood', 'NAACP',\n",
    "            'Human Rights Campaign', 'MoveOn', 'Center for American Progress'\n",
    "        ]\n",
    "        \n",
    "        self.conservative_orgs = [\n",
    "            'Heritage Foundation', 'Federalist Society', 'NRA',\n",
    "            'Americans for Prosperity', 'Cato Institute', 'Family Research Council'\n",
    "        ]\n",
    "    \n",
    "    def keyword_score(self, text: str) -> float:\n",
    "        \"\"\"Calculate ideology score based on keywords (-10 to +10).\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        liberal_count = sum(1 for kw in self.liberal_keywords if kw.lower() in text_lower)\n",
    "        conservative_count = sum(1 for kw in self.conservative_keywords if kw.lower() in text_lower)\n",
    "        \n",
    "        # Also check organization affiliations\n",
    "        liberal_count += sum(2 for org in self.liberal_orgs if org.lower() in text_lower)\n",
    "        conservative_count += sum(2 for org in self.conservative_orgs if org.lower() in text_lower)\n",
    "        \n",
    "        # Calculate score\n",
    "        total = liberal_count + conservative_count\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Score ranges from -10 (liberal) to +10 (conservative)\n",
    "        score = ((conservative_count - liberal_count) / total) * 10\n",
    "        return round(score, 2)\n",
    "    \n",
    "    def ml_classify(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Use zero-shot classification for ideology detection.\"\"\"\n",
    "        labels = [\"liberal/progressive\", \"conservative/traditional\", \"neutral/centrist\"]\n",
    "        result = zero_shot_classifier(text, labels)\n",
    "        \n",
    "        # Convert to score\n",
    "        scores = dict(zip(result['labels'], result['scores']))\n",
    "        \n",
    "        if scores.get('liberal/progressive', 0) > scores.get('conservative/traditional', 0):\n",
    "            ml_score = -10 * scores.get('liberal/progressive', 0)\n",
    "        else:\n",
    "            ml_score = 10 * scores.get('conservative/traditional', 0)\n",
    "        \n",
    "        return {\n",
    "            'scores': scores,\n",
    "            'ml_score': round(ml_score, 2),\n",
    "            'top_label': result['labels'][0],\n",
    "            'confidence': result['scores'][0]\n",
    "        }\n",
    "    \n",
    "    def classify(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Combined classification using both methods.\"\"\"\n",
    "        keyword_score = self.keyword_score(text)\n",
    "        ml_result = self.ml_classify(text)\n",
    "        \n",
    "        # Weighted average (60% ML, 40% keywords)\n",
    "        combined_score = 0.6 * ml_result['ml_score'] + 0.4 * keyword_score\n",
    "        \n",
    "        # Determine leaning\n",
    "        if combined_score < -3:\n",
    "            leaning = 'liberal'\n",
    "        elif combined_score > 3:\n",
    "            leaning = 'conservative'\n",
    "        else:\n",
    "            leaning = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'text_preview': text[:100] + '...' if len(text) > 100 else text,\n",
    "            'keyword_score': keyword_score,\n",
    "            'ml_score': ml_result['ml_score'],\n",
    "            'combined_score': round(combined_score, 2),\n",
    "            'leaning': leaning,\n",
    "            'confidence': ml_result['confidence'],\n",
    "            'ml_details': ml_result['scores']\n",
    "        }\n",
    "\n",
    "# Initialize classifier\n",
    "ideology_classifier = IdeologyClassifier()\n",
    "print(\"âœ… IdeologyClassifier initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ideology classification\n",
    "test_bios = [\n",
    "    \"\"\"\n",
    "    Sarah Johnson has been a vocal advocate for worker rights and environmental \n",
    "    protection. She serves on the board of the Sierra Club and has donated \n",
    "    to progressive causes including the ACLU.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Michael Williams is a member of the Federalist Society and advocates for \n",
    "    constitutional originalism and limited government. He has written extensively \n",
    "    about free market principles and individual liberty.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Jennifer Chen is a certified mediator with 15 years of experience in \n",
    "    commercial disputes. She focuses on finding practical solutions and \n",
    "    maintaining neutrality between all parties.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"=== Ideology Classification Results ===\")\n",
    "for i, bio in enumerate(test_bios, 1):\n",
    "    result = ideology_classifier.classify(bio)\n",
    "    \n",
    "    # Color-coded output\n",
    "    if result['leaning'] == 'liberal':\n",
    "        emoji = 'ğŸ”µ'\n",
    "    elif result['leaning'] == 'conservative':\n",
    "        emoji = 'ğŸ”´'\n",
    "    else:\n",
    "        emoji = 'âšª'\n",
    "    \n",
    "    print(f\"\\n{emoji} Mediator {i}: {result['leaning'].upper()}\")\n",
    "    print(f\"  Combined Score: {result['combined_score']} (-10=liberal, +10=conservative)\")\n",
    "    print(f\"  Keyword Score: {result['keyword_score']}\")\n",
    "    print(f\"  ML Score: {result['ml_score']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Interactive Testing Widgets\n",
    "\n",
    "Create interactive UI for real-time testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text input widget for custom analysis\n",
    "text_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter mediator bio or profile text here...',\n",
    "    description='Text:',\n",
    "    layout=widgets.Layout(width='100%', height='150px')\n",
    ")\n",
    "\n",
    "# Analysis type selector\n",
    "analysis_type = widgets.Dropdown(\n",
    "    options=['Ideology Classification', 'Affiliation Detection', 'Entity Extraction', 'All Analyses'],\n",
    "    value='All Analyses',\n",
    "    description='Analysis:',\n",
    ")\n",
    "\n",
    "# Output area\n",
    "output = widgets.Output()\n",
    "\n",
    "# Analyze button\n",
    "def run_analysis(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        text = text_input.value\n",
    "        \n",
    "        if not text.strip():\n",
    "            print(\"âš ï¸ Please enter some text to analyze.\")\n",
    "            return\n",
    "        \n",
    "        analysis = analysis_type.value\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Running: {analysis}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        if analysis in ['Ideology Classification', 'All Analyses']:\n",
    "            result = ideology_classifier.classify(text)\n",
    "            print(\"ğŸ“Š IDEOLOGY CLASSIFICATION\")\n",
    "            print(f\"  Leaning: {result['leaning'].upper()}\")\n",
    "            print(f\"  Score: {result['combined_score']}\")\n",
    "            print(f\"  Confidence: {result['confidence']:.2%}\")\n",
    "            print()\n",
    "        \n",
    "        if analysis in ['Affiliation Detection', 'All Analyses']:\n",
    "            result = detect_affiliation(text)\n",
    "            status = \"âš ï¸ POTENTIAL CONFLICT\" if result['is_affiliated'] else \"âœ… NO CONFLICT\"\n",
    "            print(f\"ğŸ” AFFILIATION DETECTION: {status}\")\n",
    "            print(f\"  Confidence: {result['confidence']:.2%}\")\n",
    "            print()\n",
    "        \n",
    "        if analysis in ['Entity Extraction', 'All Analyses']:\n",
    "            entities = ner_pipeline(text[:5000])\n",
    "            entity_groups = {}\n",
    "            for ent in entities:\n",
    "                ent_type = ent['entity_group']\n",
    "                if ent_type not in entity_groups:\n",
    "                    entity_groups[ent_type] = []\n",
    "                entity_groups[ent_type].append(ent['word'])\n",
    "            \n",
    "            print(\"ğŸ·ï¸ NAMED ENTITIES\")\n",
    "            for ent_type, words in entity_groups.items():\n",
    "                unique_words = list(set(words))\n",
    "                print(f\"  {ent_type}: {', '.join(unique_words[:10])}\")\n",
    "\n",
    "analyze_button = widgets.Button(\n",
    "    description='Analyze',\n",
    "    button_style='primary',\n",
    "    icon='search'\n",
    ")\n",
    "analyze_button.on_click(run_analysis)\n",
    "\n",
    "# Display the interface\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>ğŸ”¬ Interactive Analysis Tool</h3>\"),\n",
    "    text_input,\n",
    "    widgets.HBox([analysis_type, analyze_button]),\n",
    "    output\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Party conflict checker widget\n",
    "party_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter party name (e.g., Acme Corporation)',\n",
    "    description='Party:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "mediator_bio_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter mediator bio to check for conflicts...',\n",
    "    description='Bio:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "conflict_output = widgets.Output()\n",
    "\n",
    "def check_conflict(b):\n",
    "    with conflict_output:\n",
    "        clear_output()\n",
    "        party = party_input.value.strip()\n",
    "        bio = mediator_bio_input.value.strip()\n",
    "        \n",
    "        if not party or not bio:\n",
    "            print(\"âš ï¸ Please enter both party name and mediator bio.\")\n",
    "            return\n",
    "        \n",
    "        # Check for direct mentions\n",
    "        direct_match = party.lower() in bio.lower()\n",
    "        \n",
    "        # Extract entities and check for related organizations\n",
    "        entities = ner_pipeline(bio)\n",
    "        orgs = [e['word'] for e in entities if e['entity_group'] == 'ORG']\n",
    "        \n",
    "        # Check for semantic similarity\n",
    "        conflict_check = zero_shot_classifier(\n",
    "            f\"The mediator has connections to {party}: {bio}\",\n",
    "            [\"has conflict of interest\", \"no conflict of interest\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Conflict Check: {party}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        if direct_match:\n",
    "            print(\"ğŸš¨ DIRECT MATCH FOUND - High conflict risk!\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š AI Conflict Assessment:\")\n",
    "        print(f\"  {conflict_check['labels'][0]}: {conflict_check['scores'][0]:.2%}\")\n",
    "        \n",
    "        if orgs:\n",
    "            print(f\"\\nğŸ¢ Organizations mentioned: {', '.join(set(orgs))}\")\n",
    "\n",
    "conflict_button = widgets.Button(\n",
    "    description='Check Conflict',\n",
    "    button_style='warning',\n",
    "    icon='exclamation-triangle'\n",
    ")\n",
    "conflict_button.on_click(check_conflict)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>âš ï¸ Conflict Checker</h3>\"),\n",
    "    party_input,\n",
    "    mediator_bio_input,\n",
    "    conflict_button,\n",
    "    conflict_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Integration with Backend/Frontend\n",
    "\n",
    "### How to connect this pipeline to FairMediator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Call the existing Python scraper service\n",
    "import requests\n",
    "\n",
    "SCRAPER_SERVICE_URL = \"http://localhost:8001\"\n",
    "\n",
    "async def call_scraper_service(endpoint: str, data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Call the FairMediator Python scraper service.\n",
    "    \n",
    "    The service is located at:\n",
    "    backend/src/services/scraper/python/scraper_service.py\n",
    "    \n",
    "    Start it with: python scraper_service.py\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{SCRAPER_SERVICE_URL}{endpoint}\",\n",
    "            json=data,\n",
    "            timeout=30\n",
    "        )\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage (requires scraper service running)\n",
    "print(\"\"\"\n",
    "=== Backend Integration ===\n",
    "\n",
    "To integrate this notebook with the backend:\n",
    "\n",
    "1. Start the Python scraper service:\n",
    "   cd backend/src/services/scraper/python\n",
    "   python scraper_service.py\n",
    "\n",
    "2. The service exposes these endpoints:\n",
    "   - POST /scrape/mediator-profile\n",
    "   - POST /scrape/affiliations\n",
    "   - POST /scrape/ideology\n",
    "   - POST /scrape/bulk\n",
    "\n",
    "3. The Node.js backend connects via:\n",
    "   backend/src/services/llama/llamaClient.js\n",
    "\n",
    "4. Frontend makes API calls to:\n",
    "   POST /api/chat/enrich-mediator\n",
    "   POST /api/chat/check-conflicts\n",
    "   POST /api/chat/analyze-ideology\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export trained model for production use\n",
    "def export_model_for_production():\n",
    "    \"\"\"\n",
    "    Export the fine-tuned model for use in production.\n",
    "    \n",
    "    In production, you would:\n",
    "    1. Save the model to a directory\n",
    "    2. Load it in the Python scraper service\n",
    "    3. Or host it on HuggingFace Hub (free for public models)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Uncomment to save model after training:\n",
    "    # trainer.save_model('./production_models/affiliation_detector')\n",
    "    # tokenizer.save_pretrained('./production_models/affiliation_detector')\n",
    "    \n",
    "    print(\"\"\"\n",
    "=== Model Export Guide ===\n",
    "\n",
    "To export your trained model:\n",
    "\n",
    "1. After training, save the model:\n",
    "   trainer.save_model('./production_models/affiliation_detector')\n",
    "   tokenizer.save_pretrained('./production_models/affiliation_detector')\n",
    "\n",
    "2. Load in production:\n",
    "   from transformers import pipeline\n",
    "   classifier = pipeline(\n",
    "       'text-classification',\n",
    "       model='./production_models/affiliation_detector'\n",
    "   )\n",
    "\n",
    "3. Or push to HuggingFace Hub (free):\n",
    "   model.push_to_hub('your-username/fairmediator-affiliation')\n",
    "   tokenizer.push_to_hub('your-username/fairmediator-affiliation')\n",
    "\"\"\")\n",
    "\n",
    "export_model_for_production()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "### Recommended Enhancements\n",
    "\n",
    "1. **Collect More Training Data**\n",
    "   - Scrape real mediator profiles from legal databases\n",
    "   - Label data for affiliation/ideology manually or with weak supervision\n",
    "   - Target: 1000+ labeled examples for robust training\n",
    "\n",
    "2. **Improve NER**\n",
    "   - Fine-tune NER model on legal entities (law firms, bar associations)\n",
    "   - Add custom entity types: CASE_TYPE, JURISDICTION, CERTIFICATION\n",
    "\n",
    "3. **Add PDF/DOCX Parsing**\n",
    "   - Install: `pip install pdf-parse mammoth`\n",
    "   - Parse uploaded legal documents for automatic analysis\n",
    "\n",
    "4. **Real-time FEC Data**\n",
    "   - Integrate FEC donation API for political donation tracking\n",
    "   - API: https://api.open.fec.gov/\n",
    "\n",
    "5. **Graph-based Analysis**\n",
    "   - Build affiliation networks using NetworkX\n",
    "   - Detect hidden relationships through graph analysis\n",
    "\n",
    "6. **A/B Testing**\n",
    "   - Test different models for accuracy comparison\n",
    "   - Track user feedback on recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we built\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘     FairMediator AI Pipeline - Summary                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                          â•‘\n",
    "â•‘  âœ… Text Classification (Sentiment/Affiliation)          â•‘\n",
    "â•‘  âœ… Named Entity Recognition (Organizations, People)     â•‘\n",
    "â•‘  âœ… Zero-Shot Classification (Flexible categorization)   â•‘\n",
    "â•‘  âœ… Web Scraping (BeautifulSoup + aiohttp)               â•‘\n",
    "â•‘  âœ… Affiliation Detection Pipeline                       â•‘\n",
    "â•‘  âœ… Ideology Classification (Keywords + ML)              â•‘\n",
    "â•‘  âœ… Interactive Widgets for Testing                      â•‘\n",
    "â•‘  âœ… Backend Integration Guide                            â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•‘  All using FREE tier services:                           â•‘\n",
    "â•‘  â€¢ HuggingFace Transformers                              â•‘\n",
    "â•‘  â€¢ Ollama (local LLM)                                    â•‘\n",
    "â•‘  â€¢ Open-source scraping tools                            â•‘\n",
    "â•‘                                                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
